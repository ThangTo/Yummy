"""
AI Service - FastAPI Microservice
X·ª≠ l√Ω ·∫£nh m√≥n ƒÉn b·∫±ng nhi·ªÅu models song song
"""

from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from PIL import Image
import io
from typing import Dict, List
import uvicorn
import numpy as np

from services.model_service import ModelService
from services.prediction_service import PredictionService
from utils.image_processor import ImageProcessor

app = FastAPI(
    title="Yummy AI Service",
    description="AI Microservice for Vietnamese Food Recognition",
    version="1.0.0",
)

# CORS middleware ƒë·ªÉ cho ph√©p Node.js backend g·ªçi API
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Trong production n√™n gi·ªõi h·∫°n origin
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Kh·ªüi t·∫°o services
model_service = ModelService()
prediction_service = PredictionService()
image_processor = ImageProcessor()


@app.on_event("startup")
async def load_models():
    """
    Load t·∫•t c·∫£ models v√†o RAM khi server kh·ªüi ƒë·ªông.
    ƒê√¢y l√† k·ªπ thu·∫≠t t·ªëi ∆∞u Performance quan tr·ªçng nh·∫•t - lo·∫°i b·ªè Cold Start.
    """
    print("üöÄ System: ƒêang n·∫°p Models v√†o b·ªô nh·ªõ...")
    try:
        await model_service.load_all_models()
        loaded_models = len(model_service.models)
        print(f"‚úÖ System: ƒê√£ load {loaded_models} models. S·∫µn s√†ng ph·ª•c v·ª•!")
        
        # Log danh s√°ch models ƒë√£ load th√†nh c√¥ng
        if loaded_models > 0:
            print(f"üìã Available models: {', '.join(model_service.models.keys())}")
        else:
            print("‚ö†Ô∏è  Warning: No models loaded! Server may not function correctly.")
    except RuntimeError as e:
        # RuntimeError ƒë∆∞·ª£c raise khi kh√¥ng c√≥ model n√†o load ƒë∆∞·ª£c
        print(f"‚ùå Critical: {e}")
        print("‚ö†Ô∏è  Server will start but prediction endpoints may not work.")
        import traceback
        traceback.print_exc()
        # Kh√¥ng raise ƒë·ªÉ server v·∫´n c√≥ th·ªÉ start (ƒë·ªÉ check health endpoint)
    except Exception as e:
        print(f"‚ùå Unexpected error loading models: {e}")
        import traceback
        traceback.print_exc()
        # Kh√¥ng raise ƒë·ªÉ server v·∫´n c√≥ th·ªÉ start
        print("‚ö†Ô∏è  Server will start but some models may not be available.")


@app.get("/")
async def root():
    """Health check endpoint"""
    return {
        "status": "running",
        "service": "Yummy AI Service",
        "models_loaded": model_service.models_loaded(),
    }


@app.get("/health")
async def health_check():
    """Detailed health check"""
    return {
        "status": "healthy",
        "models": model_service.get_models_status(),
    }


@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    """
    Nh·∫≠n ·∫£nh m√≥n ƒÉn v√† ch·∫°y t·∫•t c·∫£ models song song ƒë·ªÉ d·ª± ƒëo√°n.
    
    Args:
        file: ·∫¢nh m√≥n ƒÉn (multipart/form-data)
    
    Returns:
        {
            "best_match": "T√™n m√≥n ƒÉn",
            "confidence": 0.98,
            "model_details": {
                "inception_v3": {"prediction": "Pho", "confidence": 0.95},
                "resnet152_v2": {"prediction": "Pho", "confidence": 0.92},
                "vgg19": {"prediction": "Pho", "confidence": 0.88},
                "inception_resnet_v2": {"prediction": "Pho", "confidence": 0.96},
                "xception": {"prediction": "Pho", "confidence": 0.94},
            },
            "voting_result": {...}
        }
    """
    try:
        # 1. ƒê·ªçc ·∫£nh t·ª´ RAM (kh√¥ng ghi ra ƒëƒ©a ƒë·ªÉ t·ªëi ∆∞u I/O)
        image_bytes = await file.read()
        
        # Verify image bytes kh√¥ng r·ªóng
        if not image_bytes:
            raise HTTPException(status_code=400, detail="Empty image file")
        
        print(f"üì∏ Received image - Size: {len(image_bytes)} bytes, Content-Type: {file.content_type}, Filename: {file.filename}")
        
        # Ki·ªÉm tra magic bytes ƒë·ªÉ verify image format
        # JPEG: FF D8 FF
        # PNG: 89 50 4E 47
        # GIF: 47 49 46 38
        magic_bytes = image_bytes[:4]
        is_jpeg = magic_bytes[:3] == b'\xff\xd8\xff'
        is_png = magic_bytes[:4] == b'\x89PNG'
        is_gif = magic_bytes[:4] == b'GIF8'
        
        if not (is_jpeg or is_png or is_gif):
            print(f"‚ö†Ô∏è Unknown image format - Magic bytes: {magic_bytes.hex()}")
            print(f"   First 20 bytes (hex): {image_bytes[:20].hex()}")
        
        # Th·ª≠ m·ªü image v·ªõi error handling t·ªët h∆°n
        try:
            image = Image.open(io.BytesIO(image_bytes))
            # Verify image format
            image.verify()
            # Reset image sau khi verify (verify() ƒë√≥ng image)
            image = Image.open(io.BytesIO(image_bytes))
            print(f"‚úÖ Image opened successfully - Format: {image.format}, Size: {image.size}")
        except Exception as e:
            print(f"‚ùå Error opening image: {type(e).__name__}: {e}")
            print(f"   Magic bytes (hex): {magic_bytes.hex()}")
            print(f"   First 100 bytes (hex): {image_bytes[:100].hex()}")
            raise HTTPException(
                status_code=400, 
                detail=f"Invalid image format: {str(e)}"
            )
        
        # 2. Ki·ªÉm tra c√≥ models kh√¥ng
        if not model_service.models or len(model_service.models) == 0:
            raise HTTPException(
                status_code=503,
                detail="No models available. Please check server logs for model loading errors."
            )
        
        # 3. ƒê·∫£m b·∫£o prediction_service c√≥ class names
        if not prediction_service.class_names and model_service.class_names:
            prediction_service.set_class_names(model_service.class_names)
        
        # 4. Ch·∫°y t·∫•t c·∫£ models song song (parallel inference)
        # Image s·∫Ω ƒë∆∞·ª£c preprocess ri√™ng cho t·ª´ng model trong prediction_service
        predictions = await prediction_service.predict_all_models(
            image,  # Truy·ªÅn PIL Image g·ªëc
            model_service.models,
            image_processor
        )
        
        # 5. Voting mechanism ƒë·ªÉ ch·ªçn k·∫øt qu·∫£ cu·ªëi c√πng
        voting_result = prediction_service.vote(predictions)
        
        # 6. Format response ƒë·ªÉ match v·ªõi backend expectation
        model_details_formatted = {}
        for model_name, result in predictions.items():
            model_details_formatted[model_name] = {
                "prediction": result.get("prediction", "Unknown"),
                "confidence": result.get("confidence", 0.0)
            }
        
        return {
            "best_match": voting_result["prediction"],
            "confidence": voting_result["confidence"],
            "model_details": model_details_formatted,
            "voting_result": voting_result,
        }
        
    except Exception as e:
        print(f"‚ùå Error in predict: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Prediction failed: {str(e)}")


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)

